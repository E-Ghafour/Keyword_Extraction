{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/roshan-research/hazm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! python hazm/setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from configparser import ConfigParser\n",
    "from functools import reduce\n",
    "from gensim.models import Doc2Vec\n",
    "from hazm.Embedding import SentEmbedding\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'پدرم یک کشاورز بودمثل همه که مجبور به این کار بودنالبته شغل‌شو اونطوری شروع نکردسیستم میگه در معرض فشار قرار گرفتینه، مشکلی ندارهاز مخمصه عبور می‌کنیمباید خاموشش کنم، کوپرهمشو خاموش میکنم!نباید چیزی رو خاموش کنیمبابا؟شرمنده، مورف. برگرد بخوابفکر کردم تو روحیگلم روح و اینا وجود ندارهبابابزرگ میگه تو میتونی ارواح رو بگیری...نه، این بخاطر اینکه بابابزرگدیر یا زود خودش از اونا میشه. برو بخوابداشتی خواب سقوط رو میدیدی؟برگرد برو بخواب، مورفمحصولات گندم از بین رفته بودآفت اومده بود و ما مجبور به سوزوندنش بودیمهنوز زمین ذرت داشتیمچندین قطعه مزرعه ذرت داشتیمولی بیشتر وقتا گرد و غبار می‌اومدگمونم نمی‌تونم توضیح بدمطوفان گرد و خاک دائماً می‌اومدهمین، یه عالمه غبار و کثافت...ما خودمونو با پارچه‌هایی می‌پوشوندیمبعضی وقتا روی بینی و دهانمون می‌ذاشتیمکه نتونیم گرد و خاک زیادی رو بدمیمخب زمانی که بشقاب‌ها رو می‌چیدیماونا رو وارونه روی میز می‌ذاشتیملیوان‌ها، بشقاب‌ها، همه چی رو وارونه می‌کردیمبجنبین. مورف، عجله کن...ساعت توی مزرعه، واسه سم ‌پاشیبرای منطقه به روی چشمروی میز نه، مورفبابا، میتونی اینو درست کنی؟چه بلایی سر فضاپیمام آوردی؟من نبودمبذار حدس بزنم. جناب روح این کارو کردبه قفسه کتابم ضربه میزدهمچنین کتاب‌ها رو می‌ندازه زمینکله‌خر، روح وجود خارجی ندارهدربارش تحقیق کردم، بهش میگن شَـبـحبابا، بهش بگوخب، زیادم علمی نیست، مورفخودت گفتی علم درباره پذیرفتنِچیزاییه که ما از اونا نمی‌دونیم!حالا جوابشو بدهمراقب وسایل‌هامون باشکوپخیلی خب، مورف، میخوای علمی حرف بزنی؟،حالا نگو که از روح و اینا می‌ترسینه، تو باید فراتر از این باشی...باید وقایع رو ضبط کنی، آنالیزشون کنیبعد بطور منطقی \"چرا و چگونه\" این روپیدا کنی و نتایج رو بهم نشون بدی. قبول؟قبولخیلی خبروز خوبی تو مدرسه داشته باشیوایسا،مدرسه جلسه‌ی اولیا تشکیل دادهاولیا، نه اولیای اولیاآروم پسرماون طوفان گرد و خبار نیستنسلون داره محصولاتشو می‌سوزونهآفت؟گفتن که این آخرین برداشتِمحصول واسه بامیه هست.'\n",
    "keyword_count = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalize_text = normalizer.normalize(text)\n",
    "tokenize_text = [word_tokenize(txt) for txt in sent_tokenize(normalize_text)]\n",
    "tokenize_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the candidates based on the POS tag of sequence of elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammers = [\n",
    "\"\"\"\n",
    "NP:\n",
    "        {<Ne>?<N.*>}    # Noun(s) + Noun(optional) \n",
    "        \n",
    "\"\"\",\n",
    "\n",
    "\"\"\"\n",
    "NP:\n",
    "        {<N.*><AJ.*>?}    # Noun(s) + Adjective(optional) \n",
    "        \n",
    "\"\"\"\n",
    "]\n",
    "## you can also add your own grammer to be extracted from the text..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidates(tagged, grammer):\n",
    "    keyphrase_candidate = set()\n",
    "    np_parser = nltk.RegexpParser(grammer)\n",
    "    trees = np_parser.parse_sents(tagged)\n",
    "    for tree in trees:\n",
    "        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):  # For each nounphrase\n",
    "            # Concatenate the token with a space\n",
    "            keyphrase_candidate.add(' '.join(word for word, tag in subtree.leaves()))\n",
    "    keyphrase_candidate = {kp for kp in keyphrase_candidate if len(kp.split()) <= 5}\n",
    "    keyphrase_candidate = list(keyphrase_candidate)\n",
    "    return keyphrase_candidate    \n",
    "\n",
    "all_candidates = set()\n",
    "for grammer in grammers:\n",
    "    all_candidates.update(extract_candidates(token_tag_list, grammer))\n",
    "\n",
    "all_candidates = np.array(list(all_candidates))\n",
    "print(np.array(list(all_candidates)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## load sent2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2vec_model_path = 'sent2vec-naab.model'\n",
    "sent2vec_model = SentEmbedding(sent2vec_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract  the embedding vectors from the candidates and the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_candidates_vectors = [sent2vec_model[candidate] for candidate in all_candidates]\n",
    "all_candidates_vectors[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_text_vector = sent2vec_model[normalize_text]\n",
    "whole_text_vector.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the cosine similarity of the candidates and the whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_sim_whole = cosine_similarity(all_candidates_vectors, whole_text_vector.reshape(1,-1))\n",
    "candidates_sim_whole.reshape(1,-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the cosine similarity of the each candidates to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sim_candidate = cosine_similarity(all_candidates_vectors)\n",
    "candidate_sim_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize the value of the cosine similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_sim_whole_norm = candidates_sim_whole / np.max(candidates_sim_whole)\n",
    "candidates_sim_whole_norm = 0.5 + (candidates_sim_whole_norm - np.average(candidates_sim_whole_norm)) / np.std(candidates_sim_whole_norm)\n",
    "candidates_sim_whole_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(candidate_sim_candidate, np.NaN)\n",
    "candidate_sim_candidate_norm = candidate_sim_candidate / np.nanmax(candidate_sim_candidate, axis=0)\n",
    "candidate_sim_candidate_norm = 0.5 + (candidate_sim_candidate_norm - np.nanmean(candidate_sim_candidate_norm, axis=0)) / np.nanstd(candidate_sim_candidate_norm, axis=0)\n",
    "candidate_sim_candidate_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract the proper candidates based on cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.82\n",
    "N = min(len(all_candidates), keyword_count)\n",
    "\n",
    "selected_candidates = []\n",
    "unselected_candidates = [i for i in range(len(all_candidates))]\n",
    "best_candidate = np.argmax(candidates_sim_whole_norm)\n",
    "selected_candidates.append(best_candidate)\n",
    "unselected_candidates.remove(best_candidate)\n",
    "\n",
    "\n",
    "for i in range(N-1):\n",
    "    selected_vec = np.array(selected_candidates)\n",
    "    unselected_vec = np.array(unselected_candidates)\n",
    "    \n",
    "    unselected_candidate_sim_whole_norm = candidates_sim_whole_norm[unselected_vec, :]\n",
    "    \n",
    "    dist_between = candidate_sim_candidate_norm[unselected_vec][:, selected_vec]\n",
    "    \n",
    "    if dist_between.ndim == 1:\n",
    "        dist_between = dist_between[:, np.newaxis]\n",
    "    \n",
    "    best_candidate = np.argmax(beta * unselected_candidate_sim_whole_norm - (1 - beta) * np.max(dist_between, axis = 1).reshape(-1,1))\n",
    "    best_index = unselected_candidates[best_candidate]\n",
    "    selected_candidates.append(best_index)\n",
    "    unselected_candidates.remove(best_index)\n",
    "all_candidates[selected_candidates].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
